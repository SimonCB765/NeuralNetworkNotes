<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>RNN Neuron Variants</title>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}, TeX: {equationNumbers: {autoNumber: "all"}}});
        </script>
        <script async
                src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"
                type="text/javascript">
        </script>
    </head>
    <body>
        <section>
            <h2>Notation</h2>
            <p>
                The approaches demonstrated here for overcoming the vanishing gradient problem rely on replacing
                the standard summation activation function neurons with a block that contains more complex internal
                interactions. In order to visualise these blocks there is a need for not only recurrent and
                non-recurrent edges, but also weighted and unweighted edges (with an unweighted edge having a fixed
                weight of one). Additionally, the different types of connections are most easily visualised via
                color-coding of the nodes and edges. The legend for all figures that include one of these blocks can
                be seen below. Additionally, diagrams have been simplified by treating a layer as a single
                vectorised node containing $n$ components, rather than $n$ separate nodes.
            </p>
            <figure>
                <img src="../Images/RNN/FigureLegend.jpg">
                <figcaption><b>Legend for RNN Figures with Non-standard Recurrent Activation Functions.</b></figcaption>
            </figure>
        </section>
        <section>
            <h2>The Long Short-Term Memory Block</h2>
            <p>
                One common approach used to address the vanishing gradient problem is the
                long short-term memory (LSTM) architecture. With this architecture each neuron in a recurrent layer is
                replaced by a LSTM block. In its simplest form this block will contain a single memory cell and three
                multiplicative units (the input, output and forget gates) that perform continuous analogues of write,
                read and reset operations for the cell. However, it is possible (but uncommon) to incorporate more
                than one memory cell into a block. As the same output layers can be used with LSTM networks and
                standard RNNs, LSTM blocks can simply replace standard summation neurons when desired. An example of the
                internal connections of an LSTM block can be seen below.
            </p>
            <figure>
                <img src="../Images/RNN/LSTMBlock.jpg">
                <figcaption>
                    <b>Single cell LSTM memory block.</b> The three nonlinear gates control the activation of the cell
                    through multiplications (small black circles). The input and output gates
                    multiply the input to and output of the cell respectively, while the forget gate multiplies the
                    cell's state from the previous timestep. The $f$ activation function is usually the logistic
                    function to ensure that the gate output is between 1 (gate open) and 0 (gate close). The input and
                    output functions ($g$ and $h$ respectively) are usually the tanh or logistic function, although
                    in some cass $h$ may be the identity function. Weighted connections originate from
                    outside the block (either from a previous timestep or from the layer below), while
                    unweighted (weight one) connections are all internal. Inputs to the gates are all represented as
                    vectorised nodes, with $\mathbf{c}^{l,(t-1)}$ here being a vector of the values of the memory cells
                    in block $i$ during the previous timestep. Each cell only influences its own value across timesteps,
                    there is no combining of cell values from different blocks or even from different cells in the same
                    block.
                </figcaption>
            </figure>
            <p>
                The recurrent connections from the cell to the gates are peephole connections. In a LSTM block without
                peephole connections, the gates can only see the contents of the memory cell(s) via the output of the
                block during the previous timestep. However, if the output of the block is closed, i.e. the output gate
                is close to 0, the gates can not see the contents of the memory cell(s). This lack of information may
                prove detrimental to the performance of the network. Peephole connections circumvent this problem
                by enabling the gates to see the contents of the memory cell(s) even when the output gate is closed.
                The trade-off of this potential improvement is the need to optimised additional weight parameters
                and a slight complication of the backpropagation equations.
            </p>
            <p>
                The use of multiplicative gates enables LSTM memory cells to store and access information over long
                periods of time, thereby mitigating the vanishing gradient problem. For example, while the input gate
                remains closed (i.e. has an activation near 0) the cell cannot have its stored value overwritten by new
                inputs arriving in the network. However, this stored value can be made available at any time by
                opening the output gate. This preservation of gradient can be seen in the figure below.
            </p>
            <figure>
                <img src="../Images/RNN/PreservedGradient.jpg">
                <figcaption>
                    <b>Preservation of gradient across timesteps.</b> The shading of the nodes indicates the sensitivity
                    to the gradient at each time step. Coloured nodes sensitive, while white nodes are not. The state
                    of the input, output and forget gates are indicated as open (arrows with O heads) or close (arrows
                    with - heads). The input from timestep 1 is carried along until it is 'forgotten' after timestep 4,
                    being output at timesteps 2 and 4. The input from timestep 5 is immediately stored and output, but
                    is then combined with the input from timestep 6 before being output.
                </figcaption>
            </figure>
            <p>
                In order to perform the forward and backward passes of a RNN with a LSTM recurrent layer,
                first note that representing the weights of edges entering a recurrent layer using $W^l$ and $R^l$
                matrices and the bias using $\mathbf{b}^l$ is insufficient for an LSTM layer as we now have fifteen
                sets of weighted connections entering the layer: four for each of the input, forget and output gates
                and three for the input function $g$. In order to represent these additional edges, a further $P^l$
                matrix is used to represent the peephole weights and the superscripts $i$, $f$, $o$ and $g$ are used
                to indicate that edges are destined for the input gate, forget gate,
                output gate or the input function $g$ respectively. The gives matrices like $W^{l,i}$ and
                $R^{l,g}$ to respectively represent the edges from layer $l-1$ during the current timestep to the
                input gate and the recurrent edges from layer $l$ during the previous timestep to the input activation
                function.
            </p>
            <p>
                In the following equations we assume that there is one memory cell per block. Additional memory cells
                are permitted, but complicate the equations as vectorisation becomes impractical and you need to resort
                to calculating the value of outputs sequentially. The forward pass calculations are:
                $$
                    \begin{align}
                        \mathbf{i}^{l,(t)} & = f_i(W^{l,i} \mathbf{h}^{l-1,(t)} + R^{l,i} \mathbf{h}^{l,(t-1)} +
                            P^{l,i} \mathbf{c}^{l,(t-1)} + \mathbf{b}^{l,i}) \label{inputOut} \\
                        \mathbf{f}^{l,(t)} & = f_f(W^{l,f} \mathbf{h}^{l-1,(t)} + R^{l,f} \mathbf{h}^{l,(t-1)} +
                            P^{l,f} \mathbf{c}^{l,(t-1)} + \mathbf{b}^{l,f}) \label{forgetOut} \\
                        \mathbf{c}^{l,(t)} & = \mathbf{f}^{l,(t)} \mathbf{c}^{l,(t-1)} +
                            \mathbf{i}^{l,(t)} g(W^{l,g} \mathbf{h}^{l-1,(t)} + R^{l,g} \mathbf{h}^{l,(t-1)} + \mathbf{b}^{l,g}) \label{cellOut} \\
                        \mathbf{o}^{l,(t)} & = f_o(W^{l,o} \mathbf{h}^{l-1,(t)} + R^{l,o} \mathbf{h}^{l,(t-1)} +
                            P^{l,o} \mathbf{c}^{l,(t-1)} + \mathbf{b}^{l,o}) \label{outputOut} \\
                        \mathbf{h}^{l,(t)} & = \mathbf{o}^{l,(t)} h(\mathbf{c}^{l,(t)}) \label{blockOut} \\
                    \end{align}
                $$
                The $P$ matrices are forced to be diagonal in order to ensure that the memory cell in block $i$ is not
                influenced by the value of any other cell. The states of the memory cell in each of the $n$ LSTM blocks
                in the layer can then be represented by a vector, which when multiplied by the appropriate diagonal
                $P$ matrix gives the desired vector of memory states times their peephole weights.
            </p>
            <p>
                For the LSTM backpropagation equations below we take $\mathbf{a}^{i,l,(t)}$
                to be the vector of activations of the input gates of the LSTM layer $l$ at timestep $t$, i.e. the
                summed input to the gate, with similar notation for the forget gate $f$, output gate $o$, input
                function $g$ and memory cell values $c$. The output of the gates, cell and the input function are still
                represented as above, rather than the equivalent, e.g. $\mathbf{i}^{l,(t)}$ for the output of the input
                gate rather than $\mathbf{h}^{i,l,(t)}$.
                The activation and output of an entire layer
                (LSTM or otherwise) are also still represented as $\mathbf{a}^{l,(t)}$ and $\mathbf{h}^{l,(t)}$
                respectively. We begin with the backpropagation equations for the entire block and the output gate.
                $$
                    \begin{align}
                        & \text{Block Outputs (Non-LSTM Next Layer)}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                & = (W^{l+1})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{l+1,(t)}} +
                                    (R^{l,i})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{i,l,(t+1)}} +
                                    (R^{l,f})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{f,l,(t+1)}} +
                                    (R^{l,o})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{o,l,(t+1)}} +
                                    (R^{l,g})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{g,l,(t+1)}} \\
                        & \text{Block Outputs (LSTM Next Layer)}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                & = (W^{l+1,i})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{i,l+1,(t)}} +
                                    (W^{l+1,f})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{f,l+1,(t)}} +
                                    (W^{l+1,o})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{o,l+1,(t)}} +
                                    (W^{l+1,g})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{g,l+1,(t)}} +
                                    (R^{l,i})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{i,l,(t+1)}} +
                                    (R^{l,f})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{f,l,(t+1)}} +
                                    (R^{l,o})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{o,l,(t+1)}} +
                                    (R^{l,g})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{g,l,(t+1)}} \\
                        & \text{Output Gate Activations}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{o,l,(t)}}
                                & = \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                    \frac{\partial \mathbf{h}^{l,(t)}}{\partial \mathbf{o}^{l,(t)}}
                                    \frac{\partial \mathbf{o}^{l,(t)}}{\partial \mathbf{a}^{o,l,(t)}} \nonumber \\
                              &&& = \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                    h(\mathbf{c}^{l,(t)})
                                    f'(\mathbf{a}^{o,l,(t)})
                    \end{align}
                $$
                The gradient at the block output has to take into account both the non-recurrent connections to the
                following layer $l+1$ and the recurrent connections to layer $l$ at the next timestep. For an LSTM
                layer there are four set of recurrent connections: to the input gate, forget gate, output gate and
                input function $g$. Therefore, we need to propagate the derivative back through each of these sets
                of connections in order to compute the derivative on the output of LSTM layer $l$ at timestep $t$. If
                layer $l+1$ is not an LSTM layer (i.e. is a standard recurrent layer or a non-recurrent layer), then
                there is only one set of weights to propagate the derivatives back through to layer $l$. However,
                if layer $l+1$ is an LSTM layer, then there are four sets of weights through which the derivative needs
                propagating. The derivative at the activation side of the output gate is calculated in the same
                manner as for non-recurrent layers. The only note is that
                $\frac{\partial \mathbf{h}^{l,(t)}}{\partial \mathbf{o}^{l,(t)}}$ is calculated using \eqref{blockOut}
                and $\frac{\partial \mathbf{o}^{l,(t)}}{\partial \mathbf{a}^{o,l,(t)}}$ is calculated using
                \eqref{outputOut}, noting that it is equivalent to $\mathbf{o}^{l,(t)} = f_o(\mathbf{a}^{i,l,(t)})$.
                $$
                    \begin{align}
                        & \text{Memory Cell Outputs}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t)}}
                                & = \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                        \frac{\partial \mathbf{h}^{l,(t)}}{\partial \mathbf{c}^{l,(t)}} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t+1)}}
                                        \frac{\partial \mathbf{c}^{l,(t+1)}}{\partial \mathbf{c}^{l,(t)}} +
                                    (P^{l,i})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{i,l,(t+1)}} +
                                    (P^{l,f})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{f,l,(t+1)}} +
                                    (P^{l,o})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{o,l,(t+1)}} \nonumber \\
                              &&& = \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                        \mathbf{o}^{l,(t)}
                                        h'(\mathbf{c}^{l,(t)}) +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t)}}
                                        \mathbf{f}^{l,(t+1)} +
                                    (P^{l,i})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{i,l,(t+1)}} +
                                    (P^{l,f})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{f,l,(t+1)}} +
                                    (P^{l,o})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{o,l,(t+1)}} \\
                        & \text{Memory Cell Activations (via Input Function)}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{c,l,(t)}}
                                & = \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t)}}
                                    \frac{\partial \mathbf{c}^{l,(t)}}{\partial \mathbf{a}^{g,l,(t)}} \nonumber \\
                              &&& = \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t)}}
                                    \mathbf{i}^{l,(t)} g'(\mathbf{a}^{g,l,(t)})
                    \end{align}
                $$
                The memory cell has five outgoing edges, one that contributes to computing the block output, three
                weighted ones that serve as peephole weights and one unweighted one that contributes to computing
                the cell state at the next timestep. The derivative for the cost with respect to the output of the
                memory cell therefore has five summed terms. The first term,
                $\frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}} \mathbf{o}^{l,(t)} h'(\mathbf{c}^{l,(t)})$,
                is the backpropagation of the gradient through the block circle that calculates the output of the block
                and through the $h$ function back to the cell. For this term,
                $\frac{\partial \mathbf{h}^{l,(t)}}{\partial \mathbf{c}^{l,(t)}}$ is computed from \eqref{blockOut}.
                The second term,
                $
                    \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t+1)}}
                    \frac{\partial \mathbf{c}^{l,(t+1)}}{\partial \mathbf{c}^{l,(t)}}
                $,
                comes from propagating the derivative back to the memory cell at time $t$ from the
                multiplication of the cell state with the forget gate that computes one of the inputs to the cell at
                time $t+1$. This derivative is propagated back through the edge that goes directly to the cell, not
                through the forget gate, as the path through the forget gate to the cell state is handled by
                backpropagating through the peephole edges.
                <br>
                The derivative with respect to the memory cell activations only takes into account the input function
                branch as the propagation through the input and forget gates is handled when backpropagating through
                the input and forget gates, and so would be double counted if it was included here. The gradient
                $
                    \frac{\partial \mathbf{c}^{l,(t)}}{\partial \mathbf{a}^{g,l,(t)}}
                    = \mathbf{i}^{l,(t)} g'(\mathbf{a}^{g,l,(t)})
                $
                follows from \eqref{cellOut} as \eqref{cellOut} can be rewritten as
                $\mathbf{c}^{l,(t)} = \mathbf{f}^{l,(t)} \mathbf{c}^{l,(t-1)} + \mathbf{i}^{l,(t)} g(\mathbf{a}^{i,l,(t)})$
                $$
                    \begin{align}
                        & \text{Forget Gate Activations}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{f,l,(t)}}
                                & = \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t)}}
                                    \frac{\partial \mathbf{c}^{l,(t)}}{\partial \mathbf{f}^{l,(t)}}
                                    \frac{\partial \mathbf{f}^{l,(t)}}{\partial \mathbf{a}^{f,l,(t)}} \nonumber \\
                              &&& = \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t)}}
                                    \mathbf{c}^{l,(t-1)}
                                    f'_f(\mathbf{a}^{f,l,(t)}) \\
                        & \text{Input Gate Activations}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{i,l,(t)}}
                                & = \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t)}}
                                    \frac{\partial \mathbf{c}^{l,(t)}}{\partial \mathbf{i}^{l,(t)}}
                                    \frac{\partial \mathbf{i}^{l,(t)}}{\partial \mathbf{a}^{i,l,(t)}} \nonumber \\
                              &&& = \frac{\partial \mathcal{J}}{\partial \mathbf{c}^{l,(t)}}
                                    g(\mathbf{a}^{i,l,(t)})
                                    f'_i(\mathbf{a}^{i,l,(t)}) \\
                    \end{align}
                $$
                The equations for the input and forget gate activations follow by rewriting \eqref{inputOut} and
                \eqref{forgetOut} as $\mathbf{i}^{l,(t)} = f_i(\mathbf{a}^{i,l,(t)})$ and
                $\mathbf{f}^{l,(t)} = f_f(\mathbf{a}^{f,l,(t)})$ respectively.
            </p>
            <p>
                The equations for the derivatives with respect to the weights and biases can be derived in the same
                manner as they are for a standard neural network or RNN.
            </p>
        </section>
        <section>
            <h2>Gated Recurrent Units</h2>
            <p>
                Another recurrent proposed in order to overcome the vanishing gradient problem is the gated recurrent
                unit (GRU) (<a href="../References.html#Cho2014">Ref</a>). Similar to an LSTM, GRUs contain internal
                gates to control the flow and updating of information, and ultimately what information is stored between
                timesteps. However, unlike LSTMs, GRUs do not have a mechanism for controlling the degree to which the
                state of the unit is exposed at each timestep as they lack an output gate ($h$ in the LSTM). Rather,
                they expose the entire state at each timestep and compute their output as a linear interpolation
                between the previous output of the unit at time $t-1$ and a candidate output produced by the unit at
                time $t$. In order to perform this linear interpolation, GRUs make use of three gates: a reset gate, an update
                gate and a gate to compute the candidate output. The configuration of these gates in standard GRU can
                be seen below.
            </p>
            <figure>
                <img src="../Images/RNN/GRUBlock.jpg">
                <figcaption>
                    <b>GRU Block.</b> The symbols used are the same as for the LSTM, with the $f$ gates usually the
                    logistic function and $g$ usually tanh or the logistic function. The black square takes on input
                    $i$ and computes $(1-i)$.
                </figcaption>
            </figure>
            <p>
                In this configuration, when the reset gate output is close to 0 the candidate output is largely
                dependent on the input to the unit. This makes the unit act as if it is reading the first element
                of a sequence. In contrast, the update gate controls the level of missing between the candidate output
                and the output at the previous timestep. When near 1, the output of the previous timestep is largely
                passed through without any mixing with the input at the current timestep. On the other hand, when the
                output of the update gate is near 0, the unit's output largely comprises of the candidate output. The
                update gate therefore performs a similar role as the cell in an LSTM block, as it controls what
                information accumulates in the unit over time.
            </p>
            <p>
                The equations for calculating the output of a GRU can be seen below. These are largely simpler than the
                LSTM update equations, and contain fewer parameters due to the lack of peephole weights and the
                reduction in the number of components. Similar to the LSTM equations, superscripts $r$, $z$ and
                $\widetilde{h}$ are used to indicate weights (and biases) entering the reset gate, update gate and the
                $g$ function that computes the candidate output. This gives weight matrices such as $W^{l,r}$ and
                $R^{l,z}$ that respectively represent the edges from layer $l-1$ during the current timestep to the
                reset gate and the recurrent edges from layer $l$ in previous timestep to the update gate.
                $$
                    \begin{align}
                        \mathbf{r}^{l,(t)} & = f_r(W^{l,r} \mathbf{h}^{l-1,(t)} + R^{l,r} \mathbf{h}^{l,(t-1)} +
                            \mathbf{b}^{l,r}) \label{resetOut} \\
                        \mathbf{z}^{l,(t)} & = f_z(W^{l,z} \mathbf{h}^{l-1,(t)} + R^{l,z} \mathbf{h}^{l,(t-1)} +
                            \mathbf{b}^{l,z}) \label{updateOut} \\
                        \widetilde{\mathbf{h}}^{l,(t)} & = g(W^{l,\widetilde{h}} \mathbf{h}^{l-1,(t)} +
                            R^{l,\widetilde{h}} (\mathbf{r}^{l,(t)} \odot \mathbf{h}^{l,(t-1)}) +
                            \mathbf{b}^{l,\widetilde{h}}) \label{tildeOut} \\
                        \mathbf{h}^{l,(t)} & = \mathbf{z}^{l,(t)} \odot \mathbf{h}^{l,(t-1)} +
                            (1 - \mathbf{z}^{l,(t)}) \odot \widetilde{\mathbf{h}}^{l,(t)} \label{GRUOut}
                    \end{align}
                $$
                Here $\mathbf{z}^{l,(t)}$ is multiplied by $\mathbf{h}^{l,(t-1)}$ rather than
                $\widetilde{\mathbf{h}}^{l,(t)}$. Some formulations will reverse these calculations to calculate
                $(1 - \mathbf{z}^{l,(t)}) \odot \mathbf{h}^{l,(t-1)}$ and
                $\mathbf{z}^{l,(t)} \odot \widetilde{\mathbf{h}}^{l,(t)}$. This shouldn't make a difference as the
                weight matrices (and biases) should alter in order to compute the same interpolation.
            </p>
            <p>
                For the GRU backpropagation equations below we take $\mathbf{a}^{r,l,(t)}$
                to be the vector of activations of the reset gates of the GRU layer $l$ at timestep $t$, i.e. the
                summed input to the gate, with similar notation for the update gate $z$ and candidate output
                function $\widetilde{h}$. The output of the gates and candidate output function function are still
                represented as above, rather than the equivalent, e.g. $\mathbf{r}^{l,(t)}$ is used for the output of
                the reset gate rater than $\mathbf{h}^{r,l,(t)}$.
                The activation and output of an entire layer
                (GRU or otherwise) are also still represented as $\mathbf{a}^{l,(t)}$ and $\mathbf{h}^{l,(t)}$
                respectively. We begin with the activations of the update gate and the candidate output function $g$.
                $$
                    \begin{align}
                        & \text{Update Gate Activations}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{z,l,(t)}}
                                & = \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                    \frac{\partial \mathbf{h}^{l,(t)}}{\partial \mathbf{z}^{l,(t)}}
                                    \frac{\partial \mathbf{z}^{l,(t)}}{\partial \mathbf{a}^{z,l,(t)}} \nonumber \\
                              &&& = \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                    (\mathbf{h}^{l,(t-1)} - \widetilde{\mathbf{h}}^{l,(t)})
                                    f_z'(\mathbf{a}^{z,l,(t)}) \\
                        & \text{Candidate Output Activations}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{\widetilde{h},l,(t)}}
                                & = \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                    \frac{\partial \mathbf{h}^{l,(t)}}{\partial \widetilde{\mathbf{h}}^{l,(t)}}
                                    \frac{\partial \widetilde{\mathbf{h}}^{l,(t)}}{\partial \mathbf{a}^{\widetilde{h},l,(t)}} \nonumber \\
                              &&& = \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                    (1 - \mathbf{z}^{l,(t)})
                                    g'(\mathbf{a}^{\widetilde{h},l,(t)}) \\
                    \end{align}
                $$
                The equations for the reset and update gate activations follow by rewriting \eqref{updateOut} and
                \eqref{tildeOut} as $\mathbf{z}^{l,(t)} = f_z(\mathbf{a}^{z,l,(t)})$ and
                $\widetilde{\mathbf{h}}^{l,(t)} = g(\mathbf{a}^{\widetilde{h},l,(t)})$ respectively.
                $$
                    \begin{align}
                        & \text{Reset Gate Activations}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{r,l,(t)}}
                                & = \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{\widetilde{h},l,(t)}}
                                    \frac{\partial \mathbf{a}^{\widetilde{h},l,(t)}}{\partial \mathbf{r}^{l,(t)}}
                                    \frac{\partial \mathbf{r}^{l,(t)}}{\partial \mathbf{a}^{r,l,(t)}} \nonumber \\
                              &&& = \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{\widetilde{h},l,(t)}}
                                    ((R^{l,\widetilde{h}})^{\mathsf{T}} \mathbf{h}^{l,(t-1)})
                                    f_r'(\mathbf{a}^{r,l,(t)})
                    \end{align}
                $$
                The derivation with respect to the reset gate follows from rewriting \eqref{resetOut} as
                $\mathbf{r}^{l,(t)} = f_r(\mathbf{a}^{r,l,(t)})$.
                $$
                    \begin{align}
                        & \text{Block Outputs (Non-GRU Next Layer)}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                & = (W^{l+1})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{l+1,(t)}} +
                                    (R^{l,r})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{r,l,(t+1)}} +
                                    (R^{l,z})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{z,l,(t+1)}} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t+1)}}
                                        \frac{\partial \mathbf{h}^{l,(t+1)}}{\partial \mathbf{h}^{l,(t)}} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{\widetilde{h},l,(t+1)}}
                                        \frac{\partial \mathbf{a}^{\widetilde{h},l,(t+1)}}{\partial \mathbf{h}^{l,(t)}} \nonumber \\
                              &&& = (W^{l+1})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{l+1,(t)}} +
                                    (R^{l,r})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{r,l,(t+1)}} +
                                    (R^{l,z})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{z,l,(t+1)}} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t+1)}} \mathbf{z}^{l,(t+1)} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{\widetilde{h},l,(t+1)}}
                                    ((R^{l,\widetilde{h}})^{\mathsf{T}} \mathbf{r}^{l,(t+1)}) \\
                        & \text{Block Outputs (GRU Next Layer)}
                            & \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t)}}
                                & = (W^{l+1})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{l+1,(t)}} +
                                    (R^{l,r})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{r,l,(t+1)}} +
                                    (R^{l,z})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{z,l,(t+1)}} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t+1)}}
                                        \frac{\partial \mathbf{h}^{l,(t+1)}}{\partial \mathbf{h}^{l,(t)}} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{\widetilde{h},l,(t+1)}}
                                        \frac{\partial \mathbf{a}^{\widetilde{h},l,(t+1)}}{\partial \mathbf{h}^{l,(t)}} \nonumber \\
                              &&& = (W^{r,l+1})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{r,l+1,(t)}} +
                                    (W^{z,l+1})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{z,l+1,(t)}} +
                                    (W^{\widetilde{h},l+1})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{\widetilde{h},l+1,(t)}} +
                                    (R^{l,r})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{r,l,(t+1)}} +
                                    (R^{l,z})^{\mathsf{T}} \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{z,l,(t+1)}} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{h}^{l,(t+1)}} \mathbf{z}^{l,(t+1)} +
                                    \frac{\partial \mathcal{J}}{\partial \mathbf{a}^{\widetilde{h},l,(t+1)}}
                                    ((R^{l,\widetilde{h}})^{\mathsf{T}} \mathbf{r}^{l,(t+1)})
                    \end{align}
                $$
                The output of a GRU block has five outgoing edges (in the case of a non-GRU next layer), one weighted
                edge to the next layer, weighted edges to the reset and update gates at the next timestep and unweighted
                edges that get multiplied by $\mathbf{r}^{l,(t)}$ and $\mathbf{z}^{l,(t)}$. In the case of a non-GRU
                next layer the derivative of the cost with the respect to the block output will therefore consist of
                five summed terms. In the case of the following layer containing GRUs, then there are three sets of
                edges (rather than one) through which the derivative must be propagated back, giving seven summed terms.
            </p>
            <p>
                The equations for the derivatives with respect to the weights and biases can be derived in the same
                manner as they are for a standard neural network or RNN.
            </p>
        </section>
        <section>
            <h2>Gated Recurrent Units</h2>
            <p>
                Multiple studies have shown that the performance difference between LSTMs and GRUs is minimal, with
                neither approach consistently outperforming the other (e.g.
                <a href="../References.html#GrefComparison">here</a>,
                <a href="../References.html#JozefowiczComparison">here</a> and
                <a href="../References.html#ChungComparison">here</a>). However, GRUs require fewer parameters be
                trained, so may be beneficial when there is less training data available.
            </p>
        </section>
        <p>Uncited References:</p>
        <ul>
            <li>
                <a href="../References.html#GravesThesisBook">
                    Supervised Sequence Labelling with Recurrent Neural Networks
                </a>
            </li>
            <li>
                <a href="../References.html#Graves2003">
                    Generating Sequences With Recurrent Neural Networks
                </a>
            </li>
            <li>
                <a href="../References.html#Gers2002">
                    Generating Sequences With Recurrent Neural Networks
                </a>
            </li>
            <li>
                <a href="../References.html#RNNTheory">
                    Written Memories: Understanding, Deriving and Extending the LSTM
                </a>
            </li>
        </ul>
    </body>
</html>