<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>RNN Introducton and Notation</title>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
        </script>
        <script async
                src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"
                type="text/javascript">
        </script>
    </head>
    <body>
        <p>
            If the acyclic restriction imposed on feedforward neural networks is relaxed to allow directed cyclic
            connections, the resulting network is a RNN. Unlike feedforward networks, RNNs do not assume that inputs
            (and outputs) are independent of one another, nor do they require the mapping of a fixed size input vector to
            a fixed size output vector. Rather, the input to a RNN and/or its target will be a sequence. The real-valued
            vector element $t$ in an input or output sequence can be denoted as $\mathbf{x}^{(t)}$ or $\mathbf{y}^{(t)}$
            respectively, with the corresponding sequences being
            $\left( \mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(T)} \right)$ and
            $\left( \mathbf{y}^{(1)}, \mathbf{y}^{(2)}, \ldots, \mathbf{y}^{(T)} \right)$ respectively. Sequences may be
            finite or countably infinite in length, with the maximum index of a finite sequence being $T$.
            A training set will consist of an (input, output) pair, which will take on the form (vector, sequence),
            (sequence, vector) or (sequence, sequence). Using temporal terminology, although there is no need for RNNs to
            be applied to only temporal sequence data, a sequence consists of datapoints $\mathbf{x}^{(t)}$ or
            $\mathbf{y}^{(t)}$ that arrive in a discrete sequence of timesteps indexed by $t$.
        </p>
        <p>
            The time indexing may correspond to some continuous time real-world process, e.g. frames of a video or audio
            amplitudes at fixed intervals. However, it may also be ordinal, with no true correspondence to duration, or
            even have no relation to time at all, e.g. genetic data or natural language. In the word sequence "The dog went
            to the store", $\mathbf{x}^{(1)}=\mathrm{The}$, $\mathbf{x}^{(2)}=\mathrm{dog}$, etc.
        </p>
        <p>
            While RNNs are allowed to contain cyclic edges, restrictions are placed on the edges that can participate
            in a cycle. Specifically, any edge participating in a cycle must cross timesteps. That is, a cyclic edge must
            be one that enables information to flow forwards from earlier timesteps to later ones. Cycles formed in this
            manner may be of any length. At timestep $t$, nodes with recurrent (time crossing) edges will receive input
            from the forward propagation of the current datapoint $\mathbf{x}^{(t)}$ and also from the values of the
            hidden nodes $\mathbf{h}^{(t-1)}$ from the previous timestep. The output $\hat{\mathbf{y}}^{(t)}$ at time $t$
            is then calculated from the hidden node values $\mathbf{h}^{(t)}$. Inputs $\mathbf{x}^{(t')}$ can influence
            the output $\hat{\mathbf{y}}^{(t)}$ provided that $t' <= t$, i.e. $\mathbf{x}$ does not come after
            $\hat{\mathbf{y}}$. An example of a RNN (with dashed recurrent edges) can be seen below.
        </p>
        <img src="../Images/RNN/BasicDiagram.jpg">
        <p>
            Another way to visualise a RNN is to 'unroll' the network along the timesteps, leading to a network with
            no cycles. An example of this can be seen below. This approach makes it simpler to appreciate the
            bacpropagation process in RNNs, especially when the inter-timestep dependencies are more complex.
        </p>
        <img src="../Images/RNN/Unrolled.jpg">
        <p>Uncited References:</p>
        <ul>
            <li>
                <a href="../References.html#critReviewOfRNNs">
                    A Critical Review of Recurrent Neural Networks for Sequence Learning
                </a>
            </li>
            <li>
                <a href="../References.html#GravesThesisBook">
                    Supervised Sequence Labelling with Recurrent Neural Networks
                </a>
            </li>
        </ul>
    </body>
</html>