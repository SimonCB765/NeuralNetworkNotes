<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>References</title>
    </head>
    <body>
        <ul>
            <li>General Neural Network</li>
            <li>
                <ul>
                    <li id="NNOnlineBook">
                        <a href="http://neuralnetworksanddeeplearning.com/index.html">
                            Neural Networks and Deep Learning
                        </a>
                        <ul><li>An introduction to neural networks</li></ul>
                    </li>
                    <li id="DeepLearningBook">
                        <a href="http://www.deeplearningbook.org/">Deep Learning</a>
                        <ul><li>Detailed mathematical review of deep learning</li></ul>
                    </li>
                </ul>
            </li>
            <li>RNNs</li>
            <li>
                <ul>
                    <li id="critReviewOfRNNs">
                        <a href="http://arxiv.org/abs/1506.00019">
                            A Critical Review of Recurrent Neural Networks for Sequence Learning
                        </a>
                        <ul><li>Overview of all things RNN</li></ul>
                    </li>
                    <li id="longTermDepDiff1">
                        <a href="http://www.ncbi.nlm.nih.gov/pubmed/18267787">
                            Learning long-term dependencies with gradient descent is difficult
                        </a>
                    </li>
                    <li id="longTermDepDiff2">
                        <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7321">
                            Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies
                        </a>
                    </li>
                </ul>
            </li>
            <li>Optimisation</li>
            <li>
                <ul>
                    <li id="AdaDelta">
                        <a href="https://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a>
                        <ul><li>AdaDelta paper</li></ul>
                    </li>
                    <li id="AdaGrad">
                        <a href="http://www.jmlr.org/papers/v12/duchi11a.html">
                            Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
                        </a>
                        <ul><li>AdaGrad paper</li></ul>
                    </li>
                    <li id="Adam">
                        <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>
                        <ul><li>Adam paper</li></ul>
                    </li>
                    <li id="RMSprop">
                        <a href="https://www.youtube.com/watch?v=LGA-gRkLEsI">RMSprop</a>
                        <ul><li>RMSprop lecture</li></ul>
                    </li>
                </ul>
            </li>
        </ul>
    </body>
</html>